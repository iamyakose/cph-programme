{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised learning: k-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Where are we in ML Chart?\n",
    "![MLkMeans](images/kMeansMLPaths.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Remind ourselves, again:\n",
    "The Euclidian distance in D dimensions is ...?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$d( {\\bf r_i},{\\bf r_j} ) = \\sqrt{\\sum_{k=1}^D \\left( r_{i,k} - r_{j,k} \\right)^2 } = \\sqrt{ \\| {\\bf r_i} - {\\bf r_j} \\|^2 }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## What does k-Means do?\n",
    "Minimizes a distance measure, a cost, or a _loss function_.\n",
    "\n",
    "![kmeansdistance](images/k_means_distance.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Clustering application: find patterns in unlabelled data.\n",
    "![Lloyd](images/kMeansLloydsAlgo.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to learn a good value of 'k'?\n",
    "![k_means_number_clusters](images/k_means_number_clusters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive strength\n",
    "Measuring the overlap between training and testing clustering.\n",
    "![predictivestrength](images/k_means_predictive_strength.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise 1\n",
    "Use k-Means clustering on the Iris data set. You can take this sklearn as a starting point:\n",
    "http://localhost:8888/notebooks/Week15/plot_cluster_iris.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Exercise 2\n",
    "For the best fitted (by visual inspection) case of Exercise 1, above, compare to ground truth: \n",
    "\n",
    "1. Re-label the Iris data with your (learned) clustering results, thus replacing the 'ground truth' labels. But only the labels (!)\n",
    "2. Run some known classification algorithm (trees, SVM, etc) on the result with your synthetic labels.\n",
    "3. Compare the result to results using instead the 'ground truth' labels.\n",
    "3. Conclusions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Weaknesses of k-Means part 1\n",
    "1. Centroids and Voronoi tesselation gives \"bubbly foam\" as clusters (no mixing of classes allowed).\n",
    "2. Assumes data separable in \n",
    "3. 'k' is a parameter -- we don't know the value of 'k', and it should be learned (but how?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![kmeansirisfail](images/k_means_iris_fails.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Weaknesses of k-Means part 2\n",
    "Voronoi tesselation (\"bubbly foam\") effects are more easily visualized in the \"Mouse data\" example.\n",
    "![k_means_mouse_bubbles](images/k_means_mouse_bubbles.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Weaknesses of k-Means part 3\n",
    "Computational limitations\n",
    "  - Lloyd's (brute force) is NP-hard. \n",
    "  - Accelerated methods find local minima, but that's ok for most applications.\n",
    "  - Luckily, k-Means often used for preprocessing exploration, so a once-over deal.\n",
    "\n",
    "![kdtree](images/kdtreekmeans.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise 3\n",
    "Run the k-Means clustering on the voice data. \n",
    "  - What do we want to test or achieve?\n",
    "  - What should we consider in this case? \n",
    "  - How should we do it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Conclusion, k-Means\n",
    "k-Means is good for some data, not so good for others. Variants of the k-Means (using kernel tricks) perform better sometimes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Application of k-Means\n",
    "May prove a good first estimate of classes, for example. But beware..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## If time permits:\n",
    "https://arxiv.org/pdf/1504.03849.pdf\n",
    "![timepermitting](images/timepermitting.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
