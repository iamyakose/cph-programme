{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A feature vector\n",
    "\n",
    "\"A feature vector is a vector in which each dimension $j = 1, \\dots , D$ contains a value that describes the example somehow.\"\n",
    "\n",
    "* \"A feature vector is a vector in which...\" \n",
    "  * This means that we have an unnamed vector that we're trying to define\n",
    "* \"each dimension $j = 1, \\dots , D$\"\n",
    "  * Now we know we have a variable $j$ and that $j$ contains a number of things\n",
    "  * The thing starts with the 1 and ends with D -- Oh, a second variable!\n",
    "  * What is D? ... Could be 100; let's say it's 100\n",
    "  * So D is the number of dimensions, then D must be the length! And if D is 100 then the unnamed feature vector is 100 elements long\n",
    "* \"contains a value that describes the example\"\n",
    "  * Ok, so each thing inside the unnamed vector (the thing with number $1, \\dots, D$) helps us describe one piece of example data\n",
    "  * Ok, that must mean that element $1$ is describing something different that element $2$\n",
    "  * ... I don't understand much more, let's move on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\"That value is called a *feature* and is denoted as $x^j$\"\n",
    "\n",
    "* Ok, so now we have a name for our list called $x$\n",
    "* We also heard that each single element (the ones with numbers $1, \\dots, D$) is called a **feature**\n",
    "  * And each element is uniquely identifiable by $j$: $x^j$\n",
    "  * Hey, I've seen that before! That's an index!\n",
    "  * Ahhhh, easy, so this just means that I can look at an element in the vector by it's index!\n",
    "  * ... Oh, and that element is then for some reason called a *feature*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We have a vector x of 1, ..., D elements\n",
    "D = 100\n",
    "x = np.arange(1, D + 1)\n",
    "\n",
    "# Apparently we can look up a certain element in the list given j:\n",
    "j = 1\n",
    "x[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Be careful with indexing!\n",
    "print(x[0])\n",
    "print(x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A **feature vector** is then simply just a list where we can look an element up by its index.\n",
    "\n",
    "Math: $x^1$\n",
    "\n",
    "Python:\n",
    "```python\n",
    "x[0]\n",
    "    ```\n",
    "\n",
    "Math: $x^{100}$\n",
    "\n",
    "Python: \n",
    "```python\n",
    "x[99]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature vectors as descriptions of things\n",
    "\n",
    "Let's say we have a Person class:\n",
    "\n",
    "```python\n",
    "\n",
    "class Person:\n",
    "    \n",
    "    def __init__(self, age, height):\n",
    "        self.age = age\n",
    "        self.height = height\n",
    "```\n",
    "\n",
    "A Person is described by two **features**: `age` and `height`\n",
    "\n",
    "* So a **feature vector** for a person would have 2 dimensions\n",
    "  * $x^1 = $ age\n",
    "  * $x^2 = $ height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature vectors in high dimensions\n",
    "\n",
    "* Imagine an image with the resolution `1024 x 800`\n",
    "  * How many pixels does that image have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* That image has `1024 * 800 = 819200` features!\n",
    "```python\n",
    "x[0]      # First feature\n",
    "x[819199] # Last feature\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Motivating the problem\n",
    "\n",
    "“We need to be able to predict whether a particular\n",
    "customer will stay with us. Here are the logs of customers’ interactions with our product for\n",
    "five years.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What information can we get from a log?\n",
    "\n",
    "* Timestamp\n",
    "* Number of connections\n",
    "* Frequency of connections\n",
    "* Session duration\n",
    "* ...?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The more precisely we can describe the data, the better our model gets!\n",
    "\n",
    "We want to select the information that is important and *remove* the ones that aren't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature engineering\n",
    "\n",
    "The problem of transforming raw data into a dataset is called feature engineering.\n",
    "\n",
    "Goal: describe data with *informative* features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Today: techniques to encode data in specific ways\n",
    "\n",
    "* One-hot\n",
    "* Binning\n",
    "* Word embeddings\n",
    "* Normalisation\n",
    "* Standardisation\n",
    "* Dealing with missing features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## One-hot encoding\n",
    "\n",
    "Imagine $n$ features. One-hot encoding means setting **one** feature to 1 and the rest to 0.\n",
    "\n",
    "Example:\n",
    " * Features `[Left, Right]`: `[1, 0]`\n",
    " * Features `[M, T, W, T, F, S, S]`: `[0, 0, 1, 0, 0, 0]`\n",
    " * Features `[M, F, X]`: `[0, 1, 0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x2 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "xs = [['Male'], ['Female'], ['Female']]\n",
    "encoder.fit_transform(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.fit_transform(xs).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 1., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "xs = [['Male', 1], ['Female', 3], ['Female', 2]]\n",
    "encoder.fit_transform(xs).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Binning\n",
    "\n",
    "Imagine a list of numbers, say ages, that you want to cluster into groups.\n",
    "\n",
    "Binning is to take numerical data and turn it into categories.\n",
    "\n",
    "Examples:\n",
    "* Age to groups: 0-10, 11-20, 21-30, ...\n",
    "* Speed to groups: Slow, Medium, Fast, Ultra mega fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "binner = KBinsDiscretizer(n_bins=10)\n",
    "age = np.random.randint(0, 100, (1000, 1)) # Generate 1000 random numbers\n",
    "binner.fit_transform(age).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 91.,  97., 109., 102.,  95.,  94., 106., 102.,  94., 110.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binner.fit_transform(age).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_discretization_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Discretisation strageties](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization_strategies.html#sphx-glr-auto-examples-preprocessing-plot-discretization-strategies-py) in KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Word embeddings\n",
    "\n",
    "How do we represent words?\n",
    "\n",
    "Can we make them into feature vectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Yes, yes we can!\n",
    "\n",
    "![](images/word-embeddings.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "        [0, 2, 0, 1, 0, 1, 1, 0, 1],\n",
       "        [1, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "        [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit_transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-142f5b9ce7bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Normalisation\n",
    "\n",
    "Normalization is the process of converting an actual range of values which a numerical feature can take, into a standard range of values, typically in the interval\n",
    "$[−1,1]$ or $[0,1]$.\n",
    "\n",
    "$$\\bar{x} = { x - min_x \\over max_x - min_x }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[83.7300984 , 21.28059641, 61.03700538, 59.46141786, 99.7513928 ,\n",
       "        98.52567951, 53.62516221, 18.69219941, 80.7123455 ,  6.28524729,\n",
       "         8.18487279, 30.98258078, 60.35952522, 27.05927573, 35.4994218 ,\n",
       "        56.07940954, 58.95536276, 64.07022276, 40.0083268 , 70.78951353]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.random.random((1, 20)) * 100\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32070963, 0.08151062, 0.23378875, 0.22775381, 0.38207565,\n",
       "        0.37738083, 0.20539933, 0.07159634, 0.30915079, 0.02407425,\n",
       "        0.03135035, 0.11867193, 0.23119382, 0.10364457, 0.13597269,\n",
       "        0.21479978, 0.22581548, 0.24540682, 0.15324305, 0.27114358]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "Normalizer().fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Standardisation\n",
    "Standardization (or z-score normalization) is the procedure during which the feature values are rescaled so that \n",
    "they have the properties of a standard normal distribution with μ = 0 and σ = 1, where μ \n",
    "is the mean (the average value of the feature, averaged over all examples in the dataset) and σ \n",
    "is the standard deviation from the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Standard deviation ($\\sigma$)\n",
    "\n",
    "Describes the **variance** in data. How big of a spread is there around the mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal distribution\n",
    "\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Standard_deviation_diagram.svg/1920px-Standard_deviation_diagram.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Standardisation scales AND standardises values to fit the normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "StandardScaler().fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.18383635],\n",
       "       [-1.12823768],\n",
       "       [ 0.34366774],\n",
       "       [ 0.28533461],\n",
       "       [ 1.7769943 ],\n",
       "       [ 1.73161459],\n",
       "       [ 0.06925835],\n",
       "       [-1.22406815],\n",
       "       [ 1.07210978],\n",
       "       [-1.68341196],\n",
       "       [-1.61308194],\n",
       "       [-0.76904016],\n",
       "       [ 0.31858533],\n",
       "       [-0.91429307],\n",
       "       [-0.60181272],\n",
       "       [ 0.16012218],\n",
       "       [ 0.26659888],\n",
       "       [ 0.45596684],\n",
       "       [-0.43487908],\n",
       "       [ 0.70473581]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StandardScaler().fit_transform(data.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dealing with missing values\n",
    "\n",
    "* Removing them\n",
    "  * `pd.dropna`\n",
    "* Extrapolating (imputation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Imputation techniques\n",
    "\n",
    "* Imagine a dataset where you have a lot of missing values for *one* feature\n",
    "\n",
    "\n",
    "* Imagine a dataset that you cannot hand out (NDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Imputation technique 1: Averaging\n",
    "\n",
    "Replace the missing value with the average value of the feature:\n",
    "\n",
    "$$x = {1 \\over N} x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Imputation technique 2: Sampling\n",
    "\n",
    "Imagine that the data is distributed in a certain way (for instance the normal distribution).\n",
    "You can now sample new \"*mock*\" data from that distribution.\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_missing_values_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Example: Danske Bank customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Imputation in sklearn\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7. ,  2. ,  3. ],\n",
       "       [ 4. ,  3.5,  6. ],\n",
       "       [10. ,  5. ,  9. ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit_transform([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
