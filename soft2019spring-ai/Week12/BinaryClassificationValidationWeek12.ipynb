{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RECAP!!! \n",
    "## Binary Classifiers -- Performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Derived quantities from confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Accuracy, defined as $\\frac{TP + TN}{TP + TN + FP + FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Precision, defined as $\\frac{TP}{TP + FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Recall (or sensitivity), defined as $\\frac{TP}{TP + FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are _many_ more, but these are the most central derived measures of quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multi-class Confusion Matrices\n",
    "\n",
    "![ICFM](images/confusion/threelevelconfusion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Azure Revisited for Metrics\n",
    "### Multi-class Classifiers -- Measuring predictive power on Iris data.\n",
    "<a href=\"https://studio.azureml.net/Home/ViewWorkspaceCached/1126eb934c704363bccc2801633a3351#Workspaces/Experiments/Experiment/1126eb934c704363bccc2801633a3351.f-id.e74f7178700d4943ad923b50ff71fe12/ViewExperiment\">Azure **Binary Model Performance Metrics.**</a>\n",
    "![AzureRule(d)](images/confusion/azure_ml_labs.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Receiver Operator Characteristic, ROC\n",
    "![ROC](images/confusion/roccurve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Precision-Recall Curve, P-R\n",
    "![ROC](images/confusion/precisionrecallcurve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The parameters used for ROC and PR-curves.\n",
    "![confusion_curves_params](images/confusion/confusion_curves_params.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What to use - when?\n",
    "\n",
    "* The ROC validation is better for _balanced_ (or equal) numbers of each class\n",
    "* The P-R validation is better for moderate(-to-high) class _imbalance_.\n",
    "\n",
    "The ROC curve gives too optimistic a validation result, when classes are not in balance. \n",
    "## Exercise: Can you explain why this might be? Any means you choose... but work in groups.\n",
    "![rocprtest](images/confusion/exercise_roc_vs_PR.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Balanced _versus_ Un-balanced Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For a binary classifier (as which all multi-class classifiers can be cast), a balanced data set has roughly equal numbers of both classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But if we have:\n",
    " - say 90 per cent Category 1 (non-dropouts = stayers), and\n",
    " - 10 per cent Cat 2 (dropouts = non-stayers),\n",
    "then we can severely overestimate our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### This one has Accuracy of 90%,yet classified all Cat B incorrectly !!!\n",
    "![imbalanced_confusion](images/confusion/imbalanced_confusion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A possible solution? We can try to balance by either \n",
    "- synthesize more of the sparse population by **oversampling**, or\n",
    "- remove some of the abundant population by **undersampling**, or\n",
    "- choose some smarter method of either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's look at another 'real' case from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "I'd have you download it and play yourselves, but the data are 300MB+. And the training takes 95 secs on my 6-core super mac (they gave me the low clocked cpu one).\n",
    "http://localhost:8888/notebooks/soft2019spring/AI/Week12/resampling-strategies-for-imbalanced-datasets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
