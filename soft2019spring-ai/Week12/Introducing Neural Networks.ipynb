{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introducing Neural Networks\n",
    "### Demystifying NNs ?\n",
    "We are inspired from biology to an attempt at modeling the brain for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Careful studies of biological brain tissue reveals extremely complex network behavior.\n",
    "![neurons](images/neuralnets/neurons2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Neuron Anatomy\n",
    "\n",
    "Anatomy: Single brain cell (neuron) consists roughly of\n",
    " - Nucleus, Axons, Dendrites, Synapses \n",
    "\n",
    "Functionality:\n",
    " 1. Assume action potential on dendritic tree is high (enough)\n",
    " 2. Axon fires down **\"transmission line\"**\n",
    " 3. Synaptic terminals relay signal.\n",
    " 4. It either transmits a signal, or not (binary outcome).\n",
    "![anatomy](images/neuralnets/anatomy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise 1: Network \"topology\"\n",
    "#### 1: One cell only -- make a topological abstraction from biology to a weighted graph.\n",
    "#### 2: Two cells only -- ditto.\n",
    "#### 3: Many cells only -- ditto.\n",
    "\n",
    "##### Assume, that: \n",
    " - cell neuclei are **vertices** \n",
    " - dendrites and axons are **egdes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise 2: Network \"Dynamics\"\n",
    "#### 1: draw a single neuron, and add dynamics to the egdes of your graphs from Ex.1.1\n",
    "#### 2: draw two neurons, and add dynamics to the egdes of your graphs from Ex.1.2\n",
    "#### 3: from 1 & 2 above, we get a neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Artificial Neuron -- The Perceptron\n",
    " - input values, often continuous, $(x_1, x_2, \\ldots, x_n) \\equiv \\bf x$\n",
    " - weights often continuous, $(w_1, w_2, \\ldots, w_n) \\equiv \\bf w$\n",
    " - a bias function, ${\\bf b} \\equiv b_0 {\\bf \\hat{i}}$, where ${\\bf \\hat{i}} = (1,1,\\ldots,1)$\n",
    " - weighted summation unit, $z = w_1x_1 + w_2x_2 + \\ldots w_nx_n = \\sum_{i=1}^{n} w_ix_i = \\bf w \\cdot \\bf x$\n",
    " - rectification unit, ${\\bf g}({\\bf z} + {\\bf b})$, where ${\\bf g}$ can be the Heaviside or Sigmoid or ReLU function.\n",
    "The rectification unit is of particular interest -- Jens will talk more about that. Heaviside does not allow for SGD, directly.\n",
    " \n",
    "Actually, let's draw that..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![perceptron](images/neuralnets/perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The artificial Neural Network -- The Multi-layer (feed-forward) Perceptron (MLP)\n",
    "Let's draw some more neurons and connect them. To get:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![MLP](images/neuralnets/MLP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Exercise: can you (in groups) help med correct this sloppy picture?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Math Representation\n",
    "Nothing but a function (or function approximator):\n",
    "$$y_{MLP} = f_{NN}(\\boldsymbol{z}) = {\\bf f}_3({\\bf f}_2({\\bf f}_1({\\bf x})))$$\n",
    "and where \n",
    "$${\\bf f}_l({\\bf z}) \\equiv {\\bf g}_l({\\bf W}_l{\\bf z} + {\\bf b}_l)$$\n",
    "for the $l^{\\textrm{th}}$ layer (except the input layer where we use ${\\bf x}$ for some reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## The Function Approximator\n",
    "- We have an input ${\\bf x}$, and output ${\\bf z} = {\\bf f}({\\bf w},{\\bf x},{\\bf t(g)})$. \n",
    "- We WANT to approximate the 'truth' ${\\bf g}({\\bf x})$ as close as we can.\n",
    "- We design a performance measure, a loss function, \n",
    "$$\\mathcal{L} \\propto \\|{\\bf g}({\\bf x}) - {\\bf f}({\\bf w},{\\bf x},{\\bf t(g)}) \\|^2.$$\n",
    "- We minimize/maximize/optimize the loss.\n",
    "\n",
    "### Solve such optimization problems using fx SGD or other optimizer from Python packages."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
