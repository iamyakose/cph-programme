{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beginning of this lecture is based on chapter 11 of *Automate the Boring Stuff with Python* by Al Sweigart\n",
    "\n",
    "**OBS**: To make the given links work correctly, it is asumed that you start the notebook server on your VM from within the directory `/synced_folder/lecture_notes/`. That is,\n",
    "\n",
    "```bash\n",
    "vagrant@vagrant:~$ cd /synced_folder/lecture_notes/\n",
    "vagrant@vagrant:/synced_folder/lecture_notes$ notebook\n",
    "```\n",
    "\n",
    "\n",
    "# HTML Refresher\n",
    "\n",
    "HTML files are plain text files containing *tags*, which are words enclosed in angle brackets. Tags tell the browser how to format the web page. A starting tag and closing tag can enclose some text to form an element. The text (or inner HTML) is the content between the starting and closing tags.\n",
    "\n",
    "There are many different tags in HTML. Some of these tags have extra properties in the form of attributes within the angle brackets. For example, the `<a>` tag encloses text that should be a link.\n",
    "\n",
    "Some elements have an `id` attribute that is used to uniquely identify the element in the page. You will often instruct your programs to seek out an element by its id attribute, so figuring out an element’s id attribute using the browser’s developer tools is a common task in writing web scraping programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View a Page's HTML Sources\n",
    "\n",
    "Here, I will only describe how to use Firefox' development features.\n",
    "\n",
    "To view a page's sources right click on it and choose **View page source** which opens a new tab with the HTML sources. For example, the Jupiter Notebook server, serves static files under http://127.0.0.1:8888/files. Open for example the file `example.html` from within Firefox http://127.0.0.1:8888/files/data/boliga_1050-1549_1.html.\n",
    "\n",
    "![screenshot](images/view_source.png)\n",
    "\n",
    "\n",
    "In Firefox, you can bring up the Web Developer Tools Inspector by pressing `CTRL-SHIFT-C` on Windows and Linux or by `CMD-OPTION-C` on OS X.\n",
    "\n",
    "![screenshot](images/inspector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing HTML with BeautifulSoup\n",
    "\n",
    "BeautifulSoup is a module for parsing and extracting information from HTML sources. The module’s name is bs4. In case it is not already install to your VM, install it with `pip install beautifulsoup4`. While beautifulsoup4 is the name used for installation, to import BeautifulSoup you have to use `import bs4`.\n",
    "\n",
    "According to its documentation (https://www.crummy.com/software/BeautifulSoup/) *\"Beautiful Soup parses anything you give it, and does the tree traversal stuff for you. You can tell it \"Find all the links\", or \"Find all the links of class externalLink\", or \"Find all the links whose urls match \"foo.com\", or \"Find the table heading that's got bold text, then give me that text.\"\"*\n",
    "\n",
    "\n",
    "## Creating a BeautifulSoup Object from a Local HTML File\n",
    "\n",
    "The `bs4.BeautifulSoup()` function needs to be called with a string containing the HTML it will parse. The `bs4.BeautifulSoup()` function returns is a `BeautifulSoup` object.\n",
    "\n",
    "You can load a local HTML file and pass a file object to `bs4.BeautifulSoup()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "\n",
    "\n",
    "with open('./data/example.html') as f:\n",
    "    example_html = f.read()\n",
    "    \n",
    "soup = bs4.BeautifulSoup(example_html, 'html5lib')\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a BeautifulSoup Object from a Remote HTML File\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "\n",
    "\n",
    "r = requests.get('https://github.com/datsoftlyngby/soft2017fall-business-intelligence-teaching-material')\n",
    "r.raise_for_status()\n",
    "soup = bs4.BeautifulSoup(r.text, 'html5lib')\n",
    "\n",
    "print(soup.prettify()[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding an Element with the `select()` Method\n",
    "\n",
    "You can retrieve HTML elements from a `BeautifulSoup` object by calling the `select()` method and passing a string of a CSS selector for the element you are looking for. Selectors are like regular expressions: They specify a pattern to look for, in this case, in HTML pages instead of general text strings.\n",
    "\n",
    "Common CSS selector patterns include:\n",
    "\n",
    "  * `soup.select('div')` ... selects all elements named `<div>`\n",
    "  * `soup.select('#lecturer')`  ... selects the element with an id attribute of author\n",
    "  * `soup.select('.notice')` ... selects all elements that use a CSS class attribute named notice\n",
    "  * `soup.select('div span')` ... selects all elements named ``<span>` that are within an element named `<div>`\n",
    "  * `soup.select('div > span')` ... selects all elements named `<span>` that are directly within an element named `<div>`, with no other element in between\n",
    "  * `soup.select('input[name]')` ... selects all elements named `<input>` that have a name attribute with any value\n",
    "  * `soup.select('input[type=\"button\"]')` ... selects all elements named `<input>` that have an attribute named type with value button\n",
    "  \n",
    "See more in the documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#css-selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "\n",
    "\n",
    "with open('./data/example.html') as f:\n",
    "    example_html = f.read()\n",
    "    \n",
    "soup = bs4.BeautifulSoup(example_html, 'html5lib')\n",
    "\n",
    "elems = soup.select('#lecturer')\n",
    "\n",
    "#print(soup.prettify())\n",
    "print(type(elems))\n",
    "print(len(elems))\n",
    "print(type(elems[0]))\n",
    "print(elems[0].getText())\n",
    "print(elems[0].text)\n",
    "print(str(elems[0]))\n",
    "print(elems[0].attrs)\n",
    "print(elems[0].contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_elems = soup.select('p')\n",
    "\n",
    "for el in p_elems:\n",
    "    # str(p_elems[0]), str(p_elems[1]),...\n",
    "    print(str(el))\n",
    "    print(el.getText())\n",
    "    print('------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data from an Element’s Attributes\n",
    "\n",
    "The `get()` method for Tag objects makes it simple to access attribute values from an element. The method is passed a string of an attribute name and returns that attribute’s value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "\n",
    "\n",
    "with open('./data/example.html') as f:\n",
    "    example_html = f.read()\n",
    "    \n",
    "soup = bs4.BeautifulSoup(example_html, 'html5lib')\n",
    "\n",
    "span_elem = soup.select('span')[0]\n",
    "print(str(span_elem))\n",
    "print(span_elem.get('id'))\n",
    "print(span_elem.get('some_nonexistent_addr') == None)\n",
    "print(span_elem.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example, Extract Data from a Page\n",
    "\n",
    "\n",
    "Ususally, you will use web scraping to collect information, which you cannot gather otherwise. For example, we want to collect information about sale data of flats and houses in Copenhagen.\n",
    "\n",
    "Since we cannot find an API or any other open dataset, we decide to scrape a publicly available homepage. In your exercises, you will scrape a mirror of boliga.dk/sales. During class we will consider simillar but smaller pages from within this repository. These are\n",
    "\n",
    "  * http://127.0.0.1:8888/files/data/boliga_1050-1549_1.html\n",
    "  * http://127.0.0.1:8888/files/data/boliga_1050-1549_2.html\n",
    "  * http://127.0.0.1:8888/files/data/boliga_1550-1799_1.html\n",
    "\n",
    "The first two pages list some sales in the zipcode areas 1050 to 1549. The third page lists sales in the zipcode areas 1550 to 1799.\n",
    "\n",
    "**OBS** Many web pages are not built to support high traffic or they exlicitely discourage automatic access. Keep this in mind when writing your scraping tool.\n",
    "\n",
    "\n",
    "Let's have a look at the first page http://127.0.0.1:8888/files/data/boliga_1050-1549_1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "# This code is just to inline the webpage into this notebook\n",
    "url = 'http://127.0.0.1:8888/files/data/boliga_1050-1549_1.html'\n",
    "IFrame(url, width=700, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "\n",
    "\n",
    "url = 'http://127.0.0.1:8888/files/data/boliga_1050-1549_1.html'\n",
    "r = requests.get(url)\n",
    "soup = bs4.BeautifulSoup(r.content.decode('utf-8'), 'html5lib')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's say that we want to create two CSV files. One containing all data for Copenhagen's city center (`1050-1549.csv`) and one for Vesterbro (`1550-1799.csv`). To do that, we will need a function, which scrapes all data from a single page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_housing_data(url):\n",
    "\n",
    "    data = []\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(r.content.decode('utf-8'), 'html5lib')\n",
    "\n",
    "    table = soup.find('table')\n",
    "    table_body = table.find('tbody')\n",
    "\n",
    "    rows = table_body.find_all('tr')\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "\n",
    "        # Decode address column\n",
    "        address_str = cols[0].text.strip()\n",
    "        street_str = ' '.join(address_str.split(' ')[:-3])\n",
    "        city_str = ' '.join(address_str.split(' ')[-3:])\n",
    "        zip_number = int(address_str.split(' ')[-3])\n",
    "\n",
    "        # Decode number of rooms\n",
    "        no_rooms_str = cols[1].text.strip()\n",
    "        no_rooms = int(no_rooms_str)\n",
    "        \n",
    "        # Decode selling date and type\n",
    "        size_in_sq_m_str = cols[2].text.strip()\n",
    "        size_in_sq_m = int(size_in_sq_m_str)\n",
    "\n",
    "        # Decode year of construction\n",
    "        year_of_construction_str = cols[3].text.strip()\n",
    "        year_of_construction = int(year_of_construction_str)\n",
    "        \n",
    "        # Decode price\n",
    "        price_str = cols[4].text.strip()\n",
    "        price = float(price_str)   \n",
    "\n",
    "        # Decode sales date\n",
    "        sale_date_str = cols[5].text.strip()\n",
    "\n",
    "        decoded_row = (street_str, city_str, zip_number, no_rooms,\n",
    "                       size_in_sq_m, year_of_construction, price, \n",
    "                       sale_date_str)\n",
    "        data.append(decoded_row)\n",
    "    \n",
    "    print('Scraped {} sales...'.format(len(data)))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://127.0.0.1:8888/files/data/boliga_1050-1549_1.html' \n",
    "housing_data = scrape_housing_data(base_url)\n",
    "housing_data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a CSV File with the Scraped Data \n",
    "\n",
    "After scraping the data from each HTML page, we erceive a list of tuples. Each tuple will be a line in a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def save_to_csv(data, path='./out/boliga.csv'):\n",
    "    \n",
    "    with open(path, 'w', encoding='utf-8') as output_file:\n",
    "        output_writer = csv.writer(output_file)\n",
    "        output_writer.writerow(['street', 'city', 'zipcode', \n",
    "                                'no_rooms', 'size_in_sq_m', \n",
    "                                'year_of_construction', 'price', \n",
    "                                'sale_date_str'])\n",
    "\n",
    "        for row in data:\n",
    "            output_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "out_dir = './data/out'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "    \n",
    "base_url = 'http://127.0.0.1:8888/files/data'\n",
    "urls = ['boliga_1050-1549_1.html', 'boliga_1050-1549_2.html', 'boliga_1550-1799_1.html']\n",
    "urls = [os.path.join(base_url, url) for url in urls]\n",
    "\n",
    "fst_fourty_results = scrape_housing_data(urls[0])\n",
    "snd_fourty_results = scrape_housing_data(urls[1])\n",
    "fst_results = fst_fourty_results + snd_fourty_results\n",
    "\n",
    "save_to_file = os.path.join(out_dir, os.path.basename(urls[0]).split('_')[1] + '.csv')\n",
    "save_to_csv(fst_results, save_to_file)\n",
    "\n",
    "last_results = scrape_housing_data(urls[2])\n",
    "save_to_file = os.path.join(out_dir, os.path.basename(urls[2]).split('_')[1] + '.csv')\n",
    "save_to_csv(last_results, save_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls -ltr ./data/out/1*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "head ./data/out/1050-1549.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn\n",
    "\n",
    "![](https://camo.githubusercontent.com/320b4791da998fd94e34ad4a85d44d8d5a581ca4/68747470733a2f2f732d6d656469612d63616368652d616b302e70696e696d672e636f6d2f6f726967696e616c732f39662f37332f65332f39663733653366386139353864626530336230663736663838313161353461312e676966)\n",
    "\n",
    "Now, you will expand the above example to some real data. See the assignment description at:\n",
    "https://github.com/datsoftlyngby/soft2017fall-business-intelligence-teaching-material/tree/master/assignments/assignment_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Small Detour...\n",
    "\n",
    "If you want to install a new package, such as the `tqdm` package, you can try to find it in the Anaconda repository and if it is there, you can install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "conda search tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "conda install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
