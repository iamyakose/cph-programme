{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: VADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will classify data into negative and positive statements\n",
    "Before you continue, please make sure that you have nltk installed\n",
    "http://www.nltk.org/\n",
    "\n",
    "If everything is setup correctly, the below cell should run without errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by using a text dataset from sklearn\n",
    "# This dataset contain around 18000 emails from a newsgroup\n",
    "# http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/jens/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we'll start using VADER\n",
    "# For that you first have to download and install NLTK\n",
    "# When you did that, run these two lines of code to fetch\n",
    "# the vader lexicon it'll need to classify negative/positive words\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jens/workspace/cphbusiness/soft2017fall-business-intelligence-teaching-material/lib/python3.5/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "# Here we create our sentiment analyser!\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "model = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': 0.807, 'neg': 0.012, 'neu': 0.916, 'pos': 0.072}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using model.polarity_scores() give VADER a string\n",
    "# from the newsgroup and print the results\n",
    "# What do the numbers mean?\n",
    "model.polarity_scores(newsgroups_train.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cool! Now we can see how happy something is.\n",
    "# Now extract the scores for each email\n",
    "scores = [model.polarity_scores(x) for x in newsgroups_train.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good job! But it's a little confusing with all those numbers...\n",
    "# Let's make it simpler by reducing all that data into 'H' and 'N'.\n",
    "# 'H' if the post is more happy than negative, and 'N' if the\n",
    "# post is more negative than happy.\n",
    "# Put this as an additional column in your pandas data frame.\n",
    "# We can know whether something is happy if the compound >= 0.\n",
    "# Similarly an email is negative if the compound < 0.\n",
    "def happysad(n):\n",
    "    if n >= 0:\n",
    "        return 'h'\n",
    "    else:\n",
    "        return 'n'\n",
    "    \n",
    "hs_score = [happysad(d['compound']) for d in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3341"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([sad for sad in hs_score if sad == 'n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: You can use the pandas .assign method\n",
    "# http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.assign.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll start to analyse the text and actually classify which category the\n",
    "text was contained in, based on the content of the emails. So we will have\n",
    "to predict a categorical variable, given some text.\n",
    "\n",
    "You probably noticed that the emails contains a \"target\" variable as well. This\n",
    "is a number, indicating what category the email is about. This is our predicting\n",
    "variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we have to transform text into something we can work\n",
    "# with (... numbers of course!! we love numbers!)\n",
    "# Import the TfidfVectorizer below and instantiate a new vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have a vectorizer we have to fit it to our data\n",
    "crazy_vector = vectorizer.fit_transform(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crazy_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the shape of the vector - what are you seeing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have a representation of text we can actually\n",
    "# work with (numbers!), we can start to classify the category\n",
    "# Start by importing the KMeans clustering algorithm and\n",
    "# instantiate a new KMeans model - remember to set the \n",
    "# numbers of clusters with n_clusters=X !\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=20, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that you have the KMeans classifier, fit it to your\n",
    "# tfidf vectors as your input (X) and the actual target\n",
    "# category as your output (y)\n",
    "model.fit(crazy_vector, newsgroups_train.target)\n",
    "\n",
    "# Hint: Instead of creating all these models one by one\n",
    "# you can create them as a single pipeline instead\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html#sklearn.pipeline.make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79901007601202045"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally we have to figure out how well that went.\n",
    "# Use the accuracy_score to see how many categories we\n",
    "# got right!\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(newsgroups_train.target, model.predict(crazy_vector))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
