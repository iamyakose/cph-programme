{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does our example dataset from Boliga look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're running Windows (10) you can probably use many a bash shell-for-Windows.\n",
    "# Point here being that you cannot assume that you will meet a Windows GUI in all corners of industry.\n",
    "\n",
    "# You can also do shell work by the following escape from Jupyter, \n",
    "# for example for making a directory at present working dir:\n",
    "# %%cmd\n",
    "# mkdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head '../assignment_2/boliga_stats/allzipcodes/boliga_1800-1999.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "\n",
    "wc -l '../assignment_2/boliga_stats/allzipcodes/boliga_1800-1999.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to combine many CSV files?\n",
    "\n",
    "To avoid loops and globbing (https://docs.python.org/3/library/glob.html) in your preprocessing code, you might want to consider combining all CSV files per zipcode area into a single CSV file. The resulting file is ca. 115MB large, which still fits easily into RAM.\n",
    "\n",
    "In Bash, you can combine all those files for example via:\n",
    "\n",
    "```bash\n",
    "(head -1 ./uncomp/boliga_1050-1549.csv; tail -n +2 -q ./uncomp/boliga_*.csv) > boliga_all.csv\n",
    "```\n",
    "\n",
    "#### EXERCISE: _**Discuss with your group for 1 minute and try to understand and explain what this line is doing exactly.**_\n",
    "\n",
    "If that for some reason does not work, due to _'Too many files open' error_ in MacOS fx, then, one solution could be to use a four step hack:\n",
    "  1. make two directories and copy all csv-files UP TO zipcode 4990 into one, say ```'low5000'```, and all csv-files FROM 5000 into the other, say ```'top5000'```.\n",
    "  2. then do ```(ulimit -n 512; head -1 ./boliga_stats/low5000/boliga_1050-1549.csv;  tail -n +2 -q ./boliga_stats/low5000/boliga_*.csv) > low5000.csv```.\n",
    "  3. then do ```(ulimit -n 512; head -1 ./boliga_stats/top5000/boliga_5000.csv;  tail -n +2 -q ./boliga_stats/top5000/boliga_*.csv) > top5000.csv```.\n",
    "  5. finally, gather the two large files: ```(ulimit -n 512; head -1 ./top5000.csv;  tail -n +2 -q ./*5000.csv) > ./boliga_stats/all.txt```.\n",
    "  6. clean your drive if you wish.\n",
    "\n",
    "#### EXERCISE: _**Discuss with your group for 1 minute and try to understand and explain what this line is doing exactly.**_\n",
    "\n",
    "#### EXERCISE: _**Discuss with your group a better way to do this, no need for an actual solution but at least some reflection on how to proceed.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Enabling Fast Data Analysis of Labeled Column-oriented Data\n",
    "\n",
    "  > **pandas** is a Python package providing fast, flexible, and expressive data structures designed to make working with \"relational\" or \"labeled\" data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real world data analysis in Python. ...\n",
    "\n",
    "  > **pandas** is well suited for many different kinds of data:\n",
    "\n",
    "  >  * Tabular data with heterogeneously-typed columns, as in an SQL table or Excel spreadsheet\n",
    "  >  * Ordered and unordered (not necessarily fixed-frequency) time series data.\n",
    "  >  * Arbitrary matrix data (homogeneously typed or heterogeneous) with row and column labels\n",
    "  >  * Any other form of observational / statistical data sets. The data actually need not be labeled at all to be placed into a pandas data structure\n",
    "  >  * The two primary data structures of pandas, `Series` (1-dimensional) and `DataFrame` (2-dimensional), handle the vast majority of typical use cases in finance, statistics, social science, and many areas of engineering. ...\n",
    "  > http://pandas.pydata.org/pandas-docs/stable/index.html#\n",
    "  \n",
    "\n",
    "If you rather want the really fast tour, you can read through http://pandas.pydata.org/pandas-docs/stable/10min.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A CSV file containing all BOLIGA (scraped) data\n",
    "complete_data = '../assignment_2/boliga_stats/all.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will refer to Panda's classes and functions often in code, we usually import the module as `pd`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: \n",
    "What does the ```%matplotlib notebook``` do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datatpye `Series`\n",
    "\n",
    "  > `Series` is a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). The axis labels are collectively referred to as the index.\n",
    "http://pandas.pydata.org/pandas-docs/stable/dsintro.html#series\n",
    "\n",
    "You can create a `Series` by passing a list of values, letting Pandas create a default integer index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "addresses = ['Højager 106', 'Bomosevej 4', 'Kornvej 36', 'Engblommevej 35', \n",
    "             'Bakkevej 13', 'Kildebakken 13', 'Østergårdsvej 3', \n",
    "             'Bundgarnet 84', 'Vindebyvej 39B', 'Tjørnemarken 31', \n",
    "             'Lindegårdsvej 17, 2. TH', 'Forchhammervej 1', \n",
    "             'Peder Müllersvej 7', 'Lindegårdsvej 17, 2. TV', 'Kratvej 14',\n",
    "             'Karen Jeppes Vej 4', 'Åløbet 8', 'Årbygade 21', \n",
    "             'Nordre Strandvej 64A', 'Sct Jørgensbjerg 35', 'Østergårdsvej 1', \n",
    "             'Vejlegårdsparken 46, ST. TH', 'Ved Skrænten 14', \n",
    "             'Lindegårdsvej 17, 1. TH', 'Kornvænget 2']\n",
    "zip_codes = ['3400 Hillerød', '2970 Hørsholm', '4040 Jyllinge', '3210 Vejby',\n",
    "             '3630 Jægerspris', '4230 Skælskør', '4700 Næstved', '4780 Stege',\n",
    "             '2730 Herlev', '4591 Føllenslev', '2920 Charlottenlund', \n",
    "             '4500 Nykøbing Sj', '3300 Frederiksværk', '2920 Charlottenlund', \n",
    "             '3660 Stenløse', '4653 Karise', '4573 Højby', '4400 Kalundborg', \n",
    "             '3220 Tisvildeleje', '4400 Kalundborg', '4700 Næstved', \n",
    "             '2665 Vallensbæk Strand', '3250 Gilleleje', \n",
    "             '2920 Charlottenlund', '3660 Stenløse']\n",
    "prices = [730000, 900000, 703125, 172585, 310000, 1000000, 315000, 420000,\n",
    "          599560, 144000, 285000, 180000, 2075000, 570000, 400000, 247000, \n",
    "          169000, 331608, 2000000, 600000, 1035000, 726072, 892500, 570000, \n",
    "          160000]\n",
    "\n",
    "addr_series = pd.Series(addresses)\n",
    "addr_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_series = pd.Series(zip_codes)\n",
    "prices_series = pd.Series(prices)\n",
    "\n",
    "print(zip_series)\n",
    "print(prices_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datatpye `DataFrame`\n",
    "\n",
    "\n",
    "  > `DataFrame` is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dict of Series objects. It is generally the most commonly used pandas object. http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe\n",
    "  \n",
    "  \n",
    "## Creating `DataFrame`s out of Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "addresses = ['Højager 106', 'Bomosevej 4', 'Kornvej 36', 'Engblommevej 35', \n",
    "             'Bakkevej 13', 'Kildebakken 13', 'Østergårdsvej 3', \n",
    "             'Bundgarnet 84', 'Vindebyvej 39B', 'Tjørnemarken 31', \n",
    "             'Lindegårdsvej 17, 2. TH', 'Forchhammervej 1', \n",
    "             'Peder Müllersvej 7', 'Lindegårdsvej 17, 2. TV', 'Kratvej 14',\n",
    "             'Karen Jeppes Vej 4', 'Åløbet 8', 'Årbygade 21', \n",
    "             'Nordre Strandvej 64A', 'Sct Jørgensbjerg 35', 'Østergårdsvej 1', \n",
    "             'Vejlegårdsparken 46, ST. TH', 'Ved Skrænten 14', \n",
    "             'Lindegårdsvej 17, 1. TH', 'Kornvænget 2']\n",
    "zip_codes = ['3400 Hillerød', '2970 Hørsholm', '4040 Jyllinge', '3210 Vejby',\n",
    "             '3630 Jægerspris', '4230 Skælskør', '4700 Næstved', '4780 Stege',\n",
    "             '2730 Herlev', '4591 Føllenslev', '2920 Charlottenlund', \n",
    "             '4500 Nykøbing Sj', '3300 Frederiksværk', '2920 Charlottenlund', \n",
    "             '3660 Stenløse', '4653 Karise', '4573 Højby', '4400 Kalundborg', \n",
    "             '3220 Tisvildeleje', '4400 Kalundborg', '4700 Næstved', \n",
    "             '2665 Vallensbæk Strand', '3250 Gilleleje', \n",
    "             '2920 Charlottenlund', '3660 Stenløse']\n",
    "prices = [730000, 900000, 703125, 172585, 310000, 1000000, 315000, 420000,\n",
    "          599560, 144000, 285000, 180000, 2075000, 570000, 400000, 247000, \n",
    "          169000, 331608, 2000000, 600000, 1035000, 726072, 892500, 570000, \n",
    "          160000]\n",
    "\n",
    "df = pd.DataFrame({'address': addresses, 'zip_code': zip_codes, 'price': prices}, \n",
    "                  columns=['address', 'zip_code', 'price'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating `DataFrame`s out of `Series`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'address': addr_series, 'zip_code': zip_series, 'price': prices_series}, \n",
    "                  columns=['address', 'zip_code', 'price'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.DataFrame(np.array([addr_series, zip_series, prices_series]).T, \n",
    "                  columns=['address', 'zip_code', 'price'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait, what was that `.T` thing happening?\n",
    "\n",
    "In the example above you are using `numpy` to transpose the matrix that you are creating with the expression `np.array([addr_series, zip_series, prices_series])`. Check what happens if you do not do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([addr_series, zip_series, prices_series])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data into `DataFrames` directly from CSV files\n",
    "\n",
    "Now, we use Pandas' `read_csv` function to read the downloaded CSV file directly. \n",
    "\n",
    "Note, in case you have a CSV file, which contains additional information preceding the header line you can skip those rows with the keyword argument `skiprows=`.\n",
    "\n",
    "Reading the CSV file with `read_csv` returns a DataFrame directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(complete_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting a `DataFrame`\n",
    "\n",
    "Similar as on the command-line, you can inspect the `head` and `tail` of a `DataFrame` with the corresponding methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary containing the header, column types, and the size in memory of a `DataFrame` can be printed with the `info` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a bit of summarizing statistics of your `DataFrame` can be inspected with the `describe` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE (in groups of course):\n",
    "**Find out what the rows of the result from ```df.describe``` are.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since visual inspection of numerical data is often more effective, you can plot `Series` and `DataFrames` directly. We will see more on plotting later and in the next session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "df['price_per_sq_m'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE:\n",
    "**What is plotted in the figure? Is it a meaningful visualization? Can you think of a better plot to make?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "df['price_per_sq_m'].hist(bins=5000)\n",
    "#df['price_per_sq_m'].hist(bins=5000, range=[0, 100000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE:\n",
    "**What is plotted in the figure? Is it a meaningful visualization? Can you think of a better plot to make?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "#df['price_per_sq_m'].hist(bins=5000)\n",
    "df['price_per_sq_m'].hist(bins=5000, range=[0, 100000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE:\n",
    "**What is plotted in the figure? Is it a meaningful visualization? Can you think of a better plot to make?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "cols = ['year_of_construction','price_per_sq_m']\n",
    "df[cols].plot(kind='scatter', \n",
    "              x='year_of_construction',\n",
    "              y='price_per_sq_m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection of Data in a `DataFrame`\n",
    "\n",
    "## Selection by Column Name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['address']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command ```df['address']``` is equivalent with using the column name in dot notation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can select a subset of columns, by passing their names in a collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['address', 'zip_code', 'no_rooms']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection by Indexes\n",
    "\n",
    "In the following we index the third row directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use fancy-indexing too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[3:5, 4:-1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you want to refer to elements not by position as above but by their label-location you can use the `loc` method instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[3:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boolean Indexing\n",
    "\n",
    "You can use boolean arrays for indexing too. That is, you can use boolean expressions directly for indexing and you will receive all those elements for which the expression evaluates to `True`.\n",
    "\n",
    "In the following we assign `df_bolig_zealand` to `df` as the latter is shorter and makes the programs more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['zip_code'] == '2200 København N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['zip_code'] == '2100 København Ø') | \n",
    "   (df['zip_code'] == '2200 København N')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid long chains of or (`|`) connections, you might want to specify the \"or'ed\" values in a list and just ask if cell values are in this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['zip_code'].isin(['2100 København Ø', \n",
    "                        '2200 København N', \n",
    "                        '2300 København S'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find rows, in which certain cells satisfy a certain criteria, you can construct conjunctions of respective boolean expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['zip_code'] == '2100 København Ø') & (df['price'] > 750000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to have a column with a `sell_year`, so we have to create one out of the existing data. For example as in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sell_date'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt, el in enumerate(df['sell_date'].values): \n",
    "    if cnt < 10:\n",
    "        print(el);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sell_year'] = [int(el.split('-')[-1]) for el in df['sell_date'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['zip_code'] == '2100 København Ø') & (df['sell_year'] == 2001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['zip_code'] == '2100 København Ø') & (~df['no_rooms'].isnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data\n",
    " \n",
    "At some point, you might want to work only on data that is complete, which you can do by filtering out all rows, which contain a null value in a certain field.\n",
    "Let's see how bad it is...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[(df['size_in_sq_m'].isnull()) | (df['year_of_construction'].isnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's just find the 'clean' sheet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df[(~df['address'].isnull()) & (~df['zip_code'].isnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geocoding with online APIs\n",
    "\n",
    "For using geolocation services, we have to create addresses in a form that they 'understand'. Currently, we have addresses split in two columns `address` and `zip_code`, see below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create an address, which does not contain the floor of a flat anymore. That is, we will create addresses of the form `Lille Strandstræde 20 1254 København K` for all addresses.\n",
    "\n",
    "We can do that with a list comprehension as illustrated in the following. Note, that we are splitting up the string on the comma and only retain the part of the address refering to the door of the building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_addresses = [' '.join([a.split(',')[0], z]) for a, z in df[['address', 'zip_code']].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, since we remove data from the addresses, we may now have duplicates in our `api_addresses` dataset. For example, `Schleppegrellsgade 5, 4. TH` has the same address on the door as `Schleppegrellsgade 5, 3. TH`. That is important to remember, to reduce the amount of data that you push to an API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(set(api_addresses)))\n",
    "print(len(api_addresses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could use a regular expression to filter for the value of the address. That \n",
    "\n",
    "\n",
    "In the following function, we use the Google Maps Geolocation API (https://developers.google.com/maps/documentation/geolocation/intro) to receive locations in form of latitude and longitude values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def get_location_for(address='Copenhagen'):\n",
    "    api_url = 'https://maps.googleapis.com/maps/api/geocode/json'\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(api_url, params={'sensor': 'false', \n",
    "                                          'address': address})\n",
    "        results = r.json()['results']\n",
    "    \n",
    "        location = results[0]['geometry']['location']\n",
    "        lat, lon = location['lat'], location['lng']\n",
    "    except:\n",
    "        lat, lon = None, None\n",
    "    return lat, lon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we reduce the amount of data with which we query the API even more, since we do not have a lot of time in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_data\n",
    "\n",
    "df_cphn_2000 = df[(df['zip_code'] == '2200 København N') & \n",
    "                  (df['sell_year'] == 2000)]\n",
    "\n",
    "complete_addresses = [' '.join([a.split(',')[0], z]) \n",
    "                      for a, z in df_cphn_2000[['address', 'zip_code']].values]\n",
    "# This line is the equivalent of the older\n",
    "df_cphn_2000 = df_cphn_2000.assign(complete_addresses=complete_addresses)\n",
    "# df_cphn_2000.loc[:, 'complete_addresses'] = complete_addresses\n",
    "# df_cphn_2000['complete_addresses'] = complete_addresses, which will produce a warning\n",
    "\n",
    "df_cphn_2000.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_cphn_2000['complete_addresses']))\n",
    "print(len(df_cphn_2000['complete_addresses'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_code_addresses = df_cphn_2000['complete_addresses'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = [get_location_for(a) for a in to_code_addresses]\n",
    "locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the list comprehension takes a while to complete, i.e., some minutes. Even though, the amount of data we push to the API is quite low.\n",
    "\n",
    "Using a paid plan, we could query more than 2000 addresses, see https://developers.google.com/maps/documentation/geolocation/get-api-key.\n",
    "\n",
    "However, as we have already 103,396 unique addresses for the housing sales records only on Zealand and only within the years 2000 to 2005, see `df_bolig_zealand` geocoding of those addresses would take ca. 35 minutes (`103396 / 50 / 60`). Furthermore, it would cost us ca. 50USD (`(103396 - 2500) / 1000 * 0.5`), see 'Standard Usage Limits' at https://developers.google.com/maps/documentation/geolocation/usage-limits\n",
    "\n",
    "Consequently, for bulk processing the Google API may not be the right choice for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lats, lons = zip(*locations)\n",
    "\n",
    "geocoded = pd.DataFrame({'complete_addresses': to_code_addresses,\n",
    "                         'lat': lats,\n",
    "                         'lon': lons})\n",
    "geocoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, it remains to merge the geolocation values from the `geocoded` `DataFrame` into the main `df` `DataFrame`. The straight forward method would be to iterate over each row and add the corresponding value. However, this is not efficient. Thus, read the documentation at http://pandas.pydata.org/pandas-docs/stable/merging.html and suggest a more efficient way of merging the locations into `df`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: \n",
    "Do the merge, according to the more safe way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geocoding with OSM API\n",
    "\n",
    "#### GeoPy Documentation\n",
    "\n",
    "If not targetting a geocoding API directly as in the example above, you can use a wrapper, which allows you to target various different APIs from different providers. One such wrapper is the `geopy` module, see http://geopy.readthedocs.io/en/latest/. It has to be installed to your environment, for example, via `pip install geopy`.\n",
    "\n",
    "In the following example we will use the Openstreetmap API with the help of `geopy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# The Nominatim geocoder seems to use openstreetmap.com...\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "\n",
    "def get_locations(address, zip_code):\n",
    "    try:\n",
    "        # This removes information about a flats storey\n",
    "        address_field = address.split(', ')[0]\n",
    "        # This one removes trailing letters on the city name\n",
    "        # It seems as if Openstreetmap cannot handle København H\n",
    "        # but it works with København\n",
    "        zip_field = ' '.join(zip_code.split(' ')[:-1])\n",
    "        search_address = ', '.join([address_field, zip_field])\n",
    "\n",
    "        geolocator = Nominatim()\n",
    "        location = geolocator.geocode(search_address)\n",
    "        return location.latitude, location.longitude\n",
    "    except:\n",
    "        print('Skipped geocoding of {} {}'.format(address, zip_code))\n",
    "        return None, None\n",
    "\n",
    "\n",
    "address = 'Åboulevard 34E, 4. TH'\n",
    "zip_code = '2200 København N'\n",
    "get_locations(address, zip_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_df = df[['address', 'zip_code']][:15]\n",
    "mini_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs = []\n",
    "#for _, el in df_bolig_zealand[['address', 'zip_code']][:15].iterrows():\n",
    "for _, el in mini_df[['address', 'zip_code']][:15].iterrows():\n",
    "    locs.append(get_locations(el['address'], el['zip_code']))\n",
    "locs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our  geocoding strategy was less smart. We query addresses again and again as we do not build a set of complete addresses refering to locations of doors. However, the advantage is, that we can `join` the result of the above operation directly to our current `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_df.join(pd.DataFrame(locs, columns=['lat', 'lon']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, geocoding is not really faster as before and there might exist similar restrictions as earlier depending, which precise API you use as a backend. So we have a look at even another way of geocoding our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Geocoding with OSM file\n",
    "\n",
    "Since we figured that online APIs may not be appropriate for geocoding such a large amount of addresses as we have -at least not for free and within a reasonable amount of time- we search a bit on the web and find http://download.geofabrik.de, which provides an offline dump of the Openstreetmaps data.\n",
    "\n",
    "We _could_ download a file with data corresponding to Danmark as in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "wget --directory-prefix=./data/ http://download.geofabrik.de/europe/denmark-latest.osm.bz2\n",
    "bzip2 -d ./data/denmark-latest.osm.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE \n",
    "_However, doing this from the Jupyter notebook is extremely slow. So I recommend doing the wget directly from a shell command line, and unzipping from command line as well._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls -ltrh ./data/denmark-latest.osm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "head ./data/denmark-latest.osm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting the file, we see that the data is stored in XML format. We could either set out to parse the XML file with standard modules. However, on the web we find that the following two modules implement a high-level parser for the OSM data.\n",
    "\n",
    "  * The `imposm.parser` https://imposm.org/docs/imposm.parser/latest/ is said to be efficient and quick, but\n",
    "  * `osmread` https://github.com/dezhin/osmread is really simple to use, so we go for that one here.\n",
    "\n",
    "In `osmread` documentation (https://github.com/dezhin/osmread#example-usage), you find, that you can read and process the geo-data like this:\n",
    "\n",
    "```python\n",
    "from osmread import parse_file, Way\n",
    "\n",
    "highway_count = 0\n",
    "for entity in parse_file('foo.osm.bz2'):\n",
    "    if isinstance(entity, Way) and 'highway' in entity.tags:\n",
    "        highway_count += 1\n",
    "\n",
    "print(\"%d highways found\" % highway_count)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go ahead and adapt the example from the documentation to fill a dictionary with the geo-locations of each address, which we can use as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osmread import parse_file, Node\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "postcodes = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "for entry in parse_file('./data/denmark-latest.osm'):\n",
    "    if (isinstance(entry, Node) and \n",
    "        'addr:street' in entry.tags and \n",
    "        'addr:postcode' in entry.tags and \n",
    "        'addr:housenumber' in entry.tags):\n",
    "        postcodes[entry.tags['addr:postcode']][entry.tags['addr:street']][entry.tags['addr:housenumber']] = entry.lon, entry.lat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the drawback with this solution is, that our initial OSM dataset for Danmark is 4.9GB large. That is, the dictionary, which we construct will likely fill more than a gigabyte of RAM. To implement memory friendly programs, you might want to resort to _generator functions_, see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osmread import parse_file, Node\n",
    "\n",
    "\n",
    "def decode_node_to_csv():\n",
    "    for entry in parse_file('./data/denmark-latest.osm'):\n",
    "        if (isinstance(entry, Node) and \n",
    "            'addr:street' in entry.tags and \n",
    "            'addr:postcode' in entry.tags and \n",
    "            'addr:housenumber' in entry.tags):\n",
    "\n",
    "            yield entry\n",
    "\n",
    "\n",
    "for idx, decoded_node in enumerate(decode_node_to_csv()):\n",
    "    if idx > 100:\n",
    "        break\n",
    "    print(idx, decoded_node)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait....; generators? `yield`?\n",
    "\n",
    "Actually, this is a really popular question with really many answers, which are so highly rated on Stackoverflow that we will have a look on them.\n",
    "\n",
    "https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At last...\n",
    "\n",
    "After all this intro to NumPy, Pandas, and data preprocessing lets have a look on how to combine different datasets and use our skills to gain insight in data on Copenhagen.\n",
    "\n",
    "Lets say we want to know if there is a relation between the amount of youngsters -people between 18 an 35- in various neighbourhoods of Copenhagen and the average square meter price in these neighbourhoods in a certain year.\n",
    "\n",
    "First, we get the code of each neighbourhood, which is used by Copenhagen komune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_kbh = pd.read_csv('./befkbhalderstatkode.csv')\n",
    "mask = (df_kbh.AAR == 2015) & (df_kbh.ALDER >= 18) & (df_kbh.ALDER <= 35)\n",
    "\n",
    "city_code = np.unique(df_kbh[mask].BYDEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we compute out of the komune data the amount of youngsters in each neighbourhood by summing up their amounts accross the different ages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youngsters = df_kbh[mask].groupby(['BYDEL']).ALDER.sum()\n",
    "youngsters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we combine all the data about Copenhagen komune's neighbourhoods and the amount of youngsters into a single `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbourhood_names = ['Indre By', 'Østerbro', 'Nørrebro', \n",
    "                       'Vesterbro/Kgs. Enghave', 'Valby', \n",
    "                       'Vanløse', 'Brønshøj-Husum', 'Bispebjerg', \n",
    "                       'Amager Øst', 'Amager Vest', 'Udenfor']\n",
    "zip_codes_low = [1000, 2100, 2200, 1500, 2500, 2720, 2700, 2400, \n",
    "                 2300, 2770, np.NaN]\n",
    "zip_codes_high = [1499, 2100, 2200, 1799, 2500, 2720, 2700, 2400, \n",
    "                  2300, 2770, np.NaN]\n",
    "df_kbh = pd.DataFrame({'city_code': city_code, \n",
    "                   'neighbourhood': neighbourhood_names, \n",
    "                   'zip_code_low': zip_codes_low, \n",
    "                   'zip_code_high': zip_codes_high}, \n",
    "                   columns=['city_code', 'neighbourhood', \n",
    "                            'zip_code_low', 'zip_code_high'],\n",
    "                   index=city_code\n",
    "                 )\n",
    "\n",
    "df_kbh = df_kbh.join(pd.DataFrame(youngsters))\n",
    "df_kbh.rename(columns={'ALDER': 'no_youngsters'}, inplace=True)\n",
    "df_kbh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['zip_nr'] = [int(el.split(' ')[0]) for el in df['zip_code'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we compute a list of the average price per square meter per neighbourhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sqm_prices = []\n",
    "\n",
    "for _, el in df_kbh[~df_kbh.zip_code_low.isnull()].iterrows():\n",
    "    mask = ((df.zip_nr >= el.zip_code_low) & \n",
    "            (df.zip_nr <= el.zip_code_high))\n",
    "    mean_sqm_prices.append(df[mask].price_per_sq_m.mean())\n",
    "    \n",
    "mean_sqm_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.scatter(mean_sqm_prices, youngsters.values[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks already quite unrelated, no? Just to be sure we compute the Pearson number (https://en.wikipedia.org/wiki/Pearson_correlation_coefficient). Jens will tell you a lot more about it. However, it tells us to which degree and with which likelyhood our two features are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "pearsonr(mean_sqm_prices, youngsters.values[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Links:\n",
    "\n",
    "  * https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
